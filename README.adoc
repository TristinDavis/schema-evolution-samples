= Spring cloud stream schema evolution samples image:https://travis-ci.org/viniciusccarvalho/schema-evolution-samples.svg?branch=master[]

:toc:
:toc-placement: preamble
== Introduction

The goal of this tutorial is to discuss approaches to achieve resilient data microservices (or data streams)
that can cope with the evolution of the data that flows through it.

The schema evolution topic has been discussed by https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html[Martin Kleppmann : schema-evolution-in-avro-protocol-buffers-thrift], and his book
: http://dataintensive.net/[Designing data intensive applications] has an entire chapter on the subject.

Our intention is to demonstrate how could we apply those lessons using http://cloud.spring.io/spring-cloud-stream/[Spring Cloud Stream] to create robust evolutionary data microservices.

Our proposed solution is to use a combination of a Schema Repository and specific serialization frameworks that can support different levels of schema evolution.

== The problem

In our sample application we create an imaginary sensor data event (based on the android sensor framework), which schema has evolved between two
versions. You can see the difference of each model bellow:
[source,java]
----
public class Sensor {
	private String id;
	private float temperature;
	private float velocity;
	private float acceleration;
	private float[] accelerometer;
	private float[] magneticField;
	private float[] orientation;
}
----

[source,java]
----
public class Sensor {
	private String id;
	private float internalTemperature; #<1>
	private float externalTemperature; #<2>
	private float velocity;
	private float acceleration;
	private float[] accelerometer;
	private float[] magneticField; #<3>

}
----
<1> The temperature field has been renamed to internalTemperature
<2> An extra field has been added
<3> The field orientation has been deprecated and its missing

In order to our system to be resilient to these changes it needs to be able to react
to three different changes on our model:

1. Field name change
2. Field addition
3. Field removal

Textual representations such as XML and JSON can deal quite easily with '2', as previous
versions of your services could simply ignore the new fields. Dealing with removal and alias is sometimes more
complex on those encodings.

`note this may not work, need to refine the example`

== Avro serialization recap

At it's core, avro relies on a schema definition file in order to be able to read and write data. The schema definition
for our Sensor data looks like this:

[source,json]
----
{
  "namespace" : "io.igx.android",
  "type" : "record",
  "name" : "Sensor",
  "fields" : [
    {"name":"id","type":"string"},
    {"name":"temperature", "type":"float", "default":0.0},
    {"name":"acceleration", "type":"float","default":0.0},
    {"name":"velocity","type":"float","default":0.0},
    {"name":"accelerometer","type":[
      "null",{
        "type":"array",
        "items":"float"
      }
    ]},
    {"name":"magneticField","type":[
      "null",{
        "type":"array",
        "items":"float"
      }
    ]},
    {"name":"orientation","type":[
      "null",{
        "type":"array",
        "items":"float"
      }
    ]}

  ]

}
----

When it comes to Serialize/Deserialize data we have a couple of options when using avro

=== Generated sources with specific writer

In this mode, you need to generate the source file that maps to the schema. There's very good
support for maven plugins, this along with a good IDE will make very easy to generate the sources
without polluting your codebase.

The following is a snippet of the generated `Sensor.java` class.

[source,java]
----
public class Sensor extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Sensor\",\"namespace\":\"io.igx.android\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"temperature\",\"type\":\"float\",\"default\":0.0},{\"name\":\"acceleration\",\"type\":\"float\",\"default\":0.0},{\"name\":\"velocity\",\"type\":\"float\",\"default\":0.0},{\"name\":\"accelerometer\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"float\"}]},{\"name\":\"magneticField\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"float\"}]},{\"name\":\"orientation\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"float\"}]}]}");
  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
  @Deprecated public java.lang.CharSequence id;
  @Deprecated public float temperature;
  @Deprecated public float acceleration;
  @Deprecated public float velocity;
  @Deprecated public java.util.List<java.lang.Float> accelerometer;
  @Deprecated public java.util.List<java.lang.Float> magneticField;
  @Deprecated public java.util.List<java.lang.Float> orientation;

  /**
   * Default constructor.  Note that this does not initialize fields
   * to their default values from the schema.  If that is desired then
   * one should use <code>newBuilder()</code>.
   */
  public Sensor() {}

  /**
   * All-args constructor.
   */
  public Sensor(java.lang.CharSequence id, java.lang.Float temperature, java.lang.Float acceleration, java.lang.Float velocity, java.util.List<java.lang.Float> accelerometer, java.util.List<java.lang.Float> magneticField, java.util.List<java.lang.Float> orientation) {
    this.id = id;
    this.temperature = temperature;
    this.acceleration = acceleration;
    this.velocity = velocity;
    this.accelerometer = accelerometer;
    this.magneticField = magneticField;
    this.orientation = orientation;
  }

  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call.
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return id;
    case 1: return temperature;
    case 2: return acceleration;
    case 3: return velocity;
    case 4: return accelerometer;
    case 5: return magneticField;
    case 6: return orientation;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }
----

The target class will have a `getSchema()` method that returns the original schema,
this can be handy when dealing with `SpecificDatumReaders`

You can then use the `SpecificDatumWriter` to serialize this class
[source,java]
----
Sensor sensor = Sensor.newBuilder().build();
SpecificDatumWriter<Sensor> writer = new SpecificDatumWriter<>(Sensor.class);
DataFileWriter<Sensor> dataFileWriter = new DataFileWriter<>(writer);
dataFileWriter.create(sensor.getSchema(),new File("sensors.dat"));
dataFileWriter.append(sensor);
dataFileWriter.close();
----

==== When to use this approach
This should be your preferred approach when you are the `Source` of data. When writing a
new `Source` class in Spring Cloud Stream, there's no reason why you shouldn't use generated classes.

=== No generated sources with Generic writer

Another approach that offers a great deal of flexibility while respecting the schema for type validation is
to use a `GenericRecord`. It works as a container, you can put entries on it, and
it will validate them according to the schema. With this approach you don't need to generate classes.

[source,java]
----
Schema.Parser parser = new Schema.Parser();
Schema schema = parser.parse("sensor.avsc");
GenericRecord sensor = new GenericData.Record(schema);
sensor.put("temperature",21.5);
sensor.put("acceleration",3.7);
GenericDatumWriter<GenericRecord> writer = new GenericDatumWriter<>(schema);
DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(writer);
dataFileWriter.create(schema,new File("sensors.dat"));
dataFileWriter.append(sensor);
dataFileWriter.close();
----

==== When to use this approach
This is a good approach to use on you middle transformation tier. This would give you
the maximum flexibility when it comes to schema changes, as we will see on the demo later on this tutorial.

=== No generated sources and Reflection based writer

Another approach is to have a Pojo mapped to your schema and use a `ReflectDatumWriter`.

[source,java]
----
Schema.Parser parser = new Schema.Parser();
Schema schema = parser.parse("tweet.avsc");
Tweet tweet = new Tweet();
ReflectDatumWriter<Tweet> writer = new ReflectDatumWriter<>(schema);
DataFileWriter<Tweet> dataFileWriter = new DataFileWriter<>(writer);
dataFileWriter.append(tweet);
dataFileWriter.close();
----

==== When to use this approach
This approach is good when you can't generate classes, an example is if you need
to integrate with a third party framework. Imagine if you want to use a Twitter framework
to receive tweets and just serialize them without having to deal with any mapping between
the framework type and your own type.

== Writing an Avro Codec for Spring Cloud Stream

Spring Cloud Stream uses a codec abstraction to serialize data that is written/read from the channels. The interface is listed bellow
[source,java]
----
public interface Codec {

void encode(Object object, OutputStream outputStream) throws IOException;

byte[] encode(Object object) throws IOException;

<T> T decode(InputStream inputStream, Class<T> type) throws IOException;

<T> T decode(byte[] bytes, Class<T> type) throws IOException;

}
----

Let's start with the format. Since we own both encoding and decoding parts
it means we can add more information on the wire to help us out figure out how to
read/write data.

A common pattern on binary protocols is to write a few bytes before the payload that can help us
identify the data that is about to be read. If you look at https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-Requests[Kafka message protocol] for example, it uses
ApiKey an ApiVersion as bytes in the beginning of the message.

This is where a schema repository comes in hand. As discussed by https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html[Martin Kleppmann : schema-evolution-in-avro-protocol-buffers-thrift] and also proposed on https://issues.apache.org/jira/browse/AVRO-1124[AVRO-1124].

The basic idea is that your component should register automatically the schema during startup (much like http://cloud.spring.io/spring-cloud-netflix/[Spring Cloud Eureka] does ), by doing this, you should have an unique number that identifies your schema, and you can then use it to add to the message payload.

With this in mind the `encoding` piece would look like this

[source,java]
----
@Override
public void encode(Object object, OutputStream outputStream) throws IOException {
  Schema schema = getSchema(object); #<1>
  Integer id = schemaRegistryClient.register(schema); #<2>
  DatumWriter writer = getDatumWriter(object.getClass(),schema); #<3>
  Encoder encoder = EncoderFactory.get().binaryEncoder(outputStream, null);
  outputStream.write(ByteBuffer.allocate(4).putInt(id).array());
  writer.write(object,encoder);
  encoder.flush();
}

@Override
public byte[] encode(Object o) throws IOException {
  ByteArrayOutputStream baos = new ByteArrayOutputStream();
  encode(o,baos);
  return baos.toByteArray();
}
----
<1> If we are using `GenericRecord` or a generated class, obtaining a schema is easy, since we just need to call the `getSchema` method of the object. If we
are using Reflection, than a local schema cache needs to exist. We can leverage Spring Boot Auto configuration to register all schema files and map them to
classes with the same namespace.

<2> Registering a schema will return a new id in case of a new schema or the existing id of a pre-registered schema

<3> To obtain the right `DatumWriter` we use the same logic as in <1> if it's a `GenericRecord` or `SpecificRecord` we use `GenericDatumWriter` or `SpecificDatumWriter` respectively, else we use `ReflectDatumWriter`

The decoding process is very similar, on a reverse order now

[source,java]
----
@Override
public <T> T decode(InputStream inputStream, Class<T> type) throws IOException {
	return decode(IOUtils.toByteArray(inputStream),type);
}

@Override
public <T> T decode(byte[] bytes, Class<T> type) throws IOException {
	Assert.notNull(bytes, "'bytes' cannot be null");
	Assert.notNull(bytes, "Class can not be null");
	ByteBuffer buf = ByteBuffer.wrap(bytes);
	byte[] payload = new byte[bytes.length-4];
	Integer schemaId = buf.getInt(); #<1>
  buf.get(payload); #<2>
	Schema schema = schemaRegistryClient.fetch(schemaId); #<3>
	DatumReader reader = getDatumReader(type,schema);
	Decoder decoder = DecoderFactory.get().binaryDecoder(payload,null);
	return (T) reader.read(null,decoder);
}
----
<1> First we find the schema id from the encoded data
<2> Copy the remaining (payload) bytes
<3> Retrieve the schema from the registry

== How to resolve Schemas?

=== What about local Schemas

== Our first sensor stream

== The reader/writer mismatch

== Looking beyond Avro
