= Spring cloud stream schema evolution samples
:toc:
:toc-placement: preamble
== Introduction

The goal of this tutorial is to discuss approaches to achieve resilient data microservices (or data streams)
that can cope with the evolution of the data that flows through it.

The schema evolution topic has been discussed by https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html[Martin Kleppmann : schema-evolution-in-avro-protocol-buffers-thrift], and his book
: http://dataintensive.net/[Designing data intensive applications] has an entire chapter on the subject.

Our intention is to demonstrate how could we apply those lessons using http://cloud.spring.io/spring-cloud-stream/[Spring Cloud Stream] to create robust evolutionary data microservices.

Our proposed solution is to use a combination of a Schema Repository and specific serialization frameworks that can support different levels of schema evolution.

== The problem

In our sample application we create an imaginary sensor data event (based on the android sensor framework), which schema has evolved between two
versions. You can see the difference of each model bellow:
[source,java]
----
public class Sensor {
	private String id;
	private float temperature;
	private float velocity;
	private float acceleration;
	private float[] accelerometer;
	private float[] magneticField;
	private float[] orientation;
}
----

[source,java]
----
public class Sensor {
	private String id;
	private float internalTemperature; #<1>
	private float externalTemperature; #<2>
	private float velocity;
	private float acceleration;
	private float[] accelerometer;
	private float[] magneticField; #<3>

}
----
<1> The temperature field has been renamed to internalTemperature
<2> An extra field has been added
<3> The field orientation has been deprecated and its missing

In order to our system to be resilient to these changes it needs to be able to react
to three different changes on our model:

1. Field name change
2. Field addition
3. Field removal

Textual representations such as XML and JSON can deal quite easily with '2', as previous
versions of your services could simply ignore the new fields. Dealing with removal and alias is sometimes more
complex on those encodings.

== Avro serialization recap

At it's core, avro relies on a schema definition file in order to be able to read and write data. The schema definition
for our Sensor data looks like this:

[source,json]
----
{
  "namespace" : "io.igx.android",
  "type" : "record",
  "name" : "Sensor",
  "fields" : [
    {"name":"id","type":"string"},
    {"name":"temperature", "type":"float", "default":0.0},
    {"name":"acceleration", "type":"float","default":0.0},
    {"name":"velocity","type":"float","default":0.0},
    {"name":"accelerometer","type":[
      "null",{
        "type":"array",
        "items":"float"
      }
    ]},
    {"name":"magneticField","type":[
      "null",{
        "type":"array",
        "items":"float"
      }
    ]},
    {"name":"orientation","type":[
      "null",{
        "type":"array",
        "items":"float"
      }
    ]}

  ]

}
----

When it comes to Serialize/Deserialize data we have a couple of options when using avro

=== Generated sources with specific writer

In this mode, you need to generate the source file that maps to the schema. There's very good
support for maven plugins, this along with a good IDE will make very easy to generate the sources
without polluting your codebase.

The following is a snippet of the generated `Sensor.java` class.

[source,java]
----
public class Sensor extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Sensor\",\"namespace\":\"io.igx.android\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"temperature\",\"type\":\"float\",\"default\":0.0},{\"name\":\"acceleration\",\"type\":\"float\",\"default\":0.0},{\"name\":\"velocity\",\"type\":\"float\",\"default\":0.0},{\"name\":\"accelerometer\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"float\"}]},{\"name\":\"magneticField\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"float\"}]},{\"name\":\"orientation\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"float\"}]}]}");
  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
  @Deprecated public java.lang.CharSequence id;
  @Deprecated public float temperature;
  @Deprecated public float acceleration;
  @Deprecated public float velocity;
  @Deprecated public java.util.List<java.lang.Float> accelerometer;
  @Deprecated public java.util.List<java.lang.Float> magneticField;
  @Deprecated public java.util.List<java.lang.Float> orientation;

  /**
   * Default constructor.  Note that this does not initialize fields
   * to their default values from the schema.  If that is desired then
   * one should use <code>newBuilder()</code>.
   */
  public Sensor() {}

  /**
   * All-args constructor.
   */
  public Sensor(java.lang.CharSequence id, java.lang.Float temperature, java.lang.Float acceleration, java.lang.Float velocity, java.util.List<java.lang.Float> accelerometer, java.util.List<java.lang.Float> magneticField, java.util.List<java.lang.Float> orientation) {
    this.id = id;
    this.temperature = temperature;
    this.acceleration = acceleration;
    this.velocity = velocity;
    this.accelerometer = accelerometer;
    this.magneticField = magneticField;
    this.orientation = orientation;
  }

  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call.
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return id;
    case 1: return temperature;
    case 2: return acceleration;
    case 3: return velocity;
    case 4: return accelerometer;
    case 5: return magneticField;
    case 6: return orientation;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }
----

The target class will have a `getSchema()` method that returns the original schema,
this can be handy when dealing with `SpecificDatumReaders`

You can then use the `SpecificDatumWriter` to serialize this class
[source,java]
----
Sensor sensor = Sensor.newBuilder().build();
SpecificDatumWriter<Sensor> writer = new SpecificDatumWriter<>(Sensor.class);
DataFileWriter<Sensor> dataFileWriter = new DataFileWriter<>(writer);
dataFileWriter.create(sensor.getSchema(),new File("sensors.dat"));
dataFileWriter.append(sensor);
dataFileWriter.close();
----

==== When to use this approach
This should be your preferred approach when you are the `Source` of data. When writing a
new `Source` class in Spring Cloud Stream, there's no reason why you shouldn't use generated classes.

=== No generated sources with Generic writer

Another approach that offers a great deal of flexibility while respecting the schema for type validation is
to use a `GenericRecord`. It works as a container, you can put entries on it, and
it will validate them according to the schema. With this approach you don't need to generate classes.

[source,java]
----
Schema.Parser parser = new Schema.Parser();
Schema schema = parser.parse("sensor.avsc");
GenericRecord sensor = new GenericData.Record(schema);
sensor.put("temperature",21.5);
sensor.put("acceleration",3.7);
GenericDatumWriter<GenericRecord> writer = new GenericDatumWriter<>(schema);
DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(writer);
dataFileWriter.create(schema,new File("sensors.dat"));
dataFileWriter.append(sensor);
dataFileWriter.close();
----

==== When to use this approach
This is a good approach to use on you middle transformation tier. This would give you
the maximum flexibility when it comes to schema changes, as we will see on the demo later on this tutorial.

=== No generated sources and Reflection based writer

Another approach is to have a Pojo mapped to your schema and use a `ReflectDatumWriter`.

[source,java]
----
Schema.Parser parser = new Schema.Parser();
Schema schema = parser.parse("tweet.avsc");
Tweet tweet = new Tweet();
ReflectDatumWriter<Tweet> writer = new ReflectDatumWriter<>(schema);
DataFileWriter<Tweet> dataFileWriter = new DataFileWriter<>(writer);
dataFileWriter.append(tweet);
dataFileWriter.close();
----

==== When to use this approach
This approach is good when you can't generate classes, an example is if you need
to integrate with a third party framework. Imagine if you want to use a Twitter framework
to receive tweets and just serialize them without having to deal with any mapping between
the framework type and your own type.

== Writing an Avro Codec for Spring Cloud Stream

== How to resolve Schemas?

=== What about local Schemas

== Our first sensor stream

== The reader/writer mismatch

== Looking beyond Avro
